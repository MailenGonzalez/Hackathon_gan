{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3bdf164-0186-4856-ac35-32dfc12422ea",
   "metadata": {},
   "source": [
    "## Introducción a CNNs\n",
    "\n",
    "### Referencias\n",
    "- LeCun, Y., Bengio, Y. and Hinton, G., 2015. [Deep Learning](https://www.nature.com/articles/nature14539). nature, 521(7553), pp.436-444.\n",
    "- [Ignite documentation](https://pytorch-ignite.ai/)\n",
    "- [PyTorch tutorials](https://pytorch.org/tutorials/) \n",
    "- [torchmetrics examples](https://torchmetrics.readthedocs.io/en/stable/pages/quickstart.html)\n",
    "- [visdom](https://github.com/fossasia/visdom)\n",
    "\n",
    "\n",
    "![pytorch cheatsheet](./figs/pytorch-cheat.jpeg \"pytorch cheatsheet\")\n",
    "\n",
    "Figure from [pytorch forum](https://discuss.pytorch.org/t/pytorch-cheat-sheet/72016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f956227c-0b45-4866-84e1-cb8738a5ad87",
   "metadata": {},
   "source": [
    "### Que pasos debemos seguir para entrenar una CNN?\n",
    "- Cargar datos, transformarlos en formato necesario para pytorch. $t_{train}$ va a tener transformaciones diferentes a $t_{test}$, por que? \n",
    "- A los datos de entrenamiento debemos dividirlos una vez mas para usar elementos de validación durante el entrenamiento.\n",
    "- Definir arquitectura de nuestra red (Ver models.py)\n",
    "- Definir métodos funciones de error, optimizador y métricas de evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa8e11a-5203-488a-8770-022643eb6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda72361-feb0-4adf-b02e-49f33838733a",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16556eba-304f-415e-b617-3643abb730aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as tfs\n",
    "from torch.utils import data\n",
    "import PIL\n",
    "\n",
    "transforms_train = [\n",
    "    tfs.RandomHorizontalFlip(p=0.7),\n",
    "    tfs.RandomAffine(0, scale=(0.7, 1.0)),\n",
    "    tfs.Resize((64, 64)),\n",
    "    tfs.Grayscale(1),\n",
    "    tfs.Lambda(lambda x: PIL.ImageOps.invert(x)),\n",
    "    tfs.ToTensor(),\n",
    "]\n",
    "\n",
    "transforms_test = [\n",
    "    tfs.RandomHorizontalFlip(p=0.7),\n",
    "    tfs.RandomAffine(0, scale=(0.7, 1.0)),\n",
    "    tfs.Resize((64, 64)),\n",
    "    tfs.Grayscale(1),\n",
    "    tfs.Lambda(lambda x: PIL.ImageOps.invert(x)),\n",
    "    tfs.ToTensor(),\n",
    "]\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    \"../data/train/\", transform=tfs.Compose(transforms_train)\n",
    ")\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    \"../data/test/\", transform=tfs.Compose(transforms_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300ce4e-5502-4abb-b250-7ddb8b13e13d",
   "metadata": {},
   "source": [
    "## Ahora tenemos que definir modelo, función de error, optimizador, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9500d95-2b05-4489-ac70-7a1f14abf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from models import _C\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from aux import create_plot_window\n",
    "import numpy as np\n",
    "\n",
    "from ignite.contrib.handlers.tensorboard_logger import (\n",
    "    global_step_from_engine,\n",
    "    GradsHistHandler,\n",
    "    GradsScalarHandler,\n",
    "    TensorboardLogger,\n",
    "    WeightsHistHandler,\n",
    "    WeightsScalarHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f660123-51e0-46a1-a18a-dc96d9d82b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06798403-9ecd-4aaa-b02f-40dc3184c9e6",
   "metadata": {},
   "source": [
    "### Que dispositivo tenemos disponible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2096dc82-3a49-4f6f-b4be-527b7872adca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo disponible: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Dispositivo disponible: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d560e-f0a4-40e6-a8d8-72662606a7df",
   "metadata": {},
   "source": [
    "### Como es nuestro modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94754971-9c27-4dcb-9a25-65d493eaf8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load models.py\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class _C(nn.Module):\n",
    "    \"\"\"Classifier class\"\"\"\n",
    "\n",
    "    def __init__(self, input_h_w=28):\n",
    "        super(_C, self).__init__()\n",
    "        self.input_height = input_h_w\n",
    "        self.input_width = input_h_w\n",
    "        self.input_dim = 1\n",
    "        self.output_dim = 11\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_height // 4) * (self.input_width // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            nn.Softmax(1),\n",
    "        )\n",
    "        # utils.initialize_weights(self)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        x = self.conv(input_)\n",
    "        x = x.view(-1, 128 * (self.input_height // 4) * (self.input_width // 4))\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0125722b-b6c4-4853-9e29-93c278da4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    '''Funcion de inicialización. Creamos el modelo, definimos el optimizador y la función de perdida'''\n",
    "    model = _C(input_h_w=64).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c224727d-6ac6-441e-be8c-3c9572b010ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataflow(dataset, train_idx, val_idx):\n",
    "    ''' Funcion de inicialización para el dataset '''\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler, drop_last=True)\n",
    "    val_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler, drop_last=True)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91850398-3cdc-4c59-93f7-072dc1c41888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31a65e1-8bfc-4d42-a2a0-d91c7ac88e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b7c0fe-08c0-47ab-8a51-11e3d96e0c93",
   "metadata": {},
   "source": [
    "#### Vamos a ver como usar ignite para entrenamiento supervisado, no se olviden de ver el dashboard the TensorBoard en [http://localhost:6000/](http://localhost:6000/)\n",
    "\n",
    "#### en una consola externa, en la carpeta Day-x ejecutar: \n",
    "##### tensorboard --logdir=./output --port 6006  --host 0.0.0.0 --load_fast=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5bffa9-371b-4666-aaf8-e9d83e5f545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, max_epochs=20, mode='training'):\n",
    "\n",
    "    train_results = []\n",
    "    val_results = []\n",
    "\n",
    "    model, optimizer, criterion = initialize()\n",
    "\n",
    "    now = datetime.now()\n",
    "    logdir = \"output/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "    writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "    trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "    evaluator = create_supervised_evaluator(model, metrics={\"Accuracy\": Accuracy(), \"Loss\": Loss(criterion)}, device=device,)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(trainer):\n",
    "        evaluator.run(train_loader)\n",
    "        metrics = evaluator.state.metrics.copy()\n",
    "        train_results.append(metrics)\n",
    "        print(f\"Training Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['Accuracy']:.2f} Avg loss: {metrics['Loss']:.2f}\")\n",
    "        \n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(trainer):\n",
    "        evaluator.run(val_loader)\n",
    "        metrics = evaluator.state.metrics.copy()\n",
    "        val_results.append(metrics)\n",
    "        \n",
    "        writer.add_scalars(f'{mode}/Accuracy',{'val':val_results[trainer.state.epoch - 1]['Accuracy'],\n",
    "                                         'train':train_results[trainer.state.epoch - 1]['Accuracy']}, trainer.state.epoch)\n",
    "        \n",
    "        writer.add_scalars(f'{mode}/Loss',{'val':val_results[trainer.state.epoch - 1]['Loss'],\n",
    "                                         'train':train_results[trainer.state.epoch - 1]['Loss']}, trainer.state.epoch)\n",
    "\n",
    "\n",
    "    trainer.run(train_loader, max_epochs=max_epochs)\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "    return model, train_results, val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8cb64f-100b-4180-af6b-84850ae6811b",
   "metadata": {},
   "source": [
    "## Particionamos los datos en Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd9edc9-ed62-477a-83cb-7afa59863f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = data.random_split(train_dataset, [867, 96])\n",
    "train_idx, val_idx = splits[0].indices, splits[1].indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83304471-ce81-4f2a-85fb-81c35a7dc2ee",
   "metadata": {},
   "source": [
    "## Entrenamos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e1ffb-3c08-4009-8ce5-6552d726a35b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results - Epoch[1] Avg accuracy: 0.61 Avg loss: 2.26\n",
      "Training Results - Epoch[2] Avg accuracy: 0.69 Avg loss: 2.01\n",
      "Training Results - Epoch[3] Avg accuracy: 0.73 Avg loss: 1.87\n",
      "Training Results - Epoch[4] Avg accuracy: 0.76 Avg loss: 1.80\n",
      "Training Results - Epoch[5] Avg accuracy: 0.76 Avg loss: 1.79\n",
      "Training Results - Epoch[6] Avg accuracy: 0.78 Avg loss: 1.76\n",
      "Training Results - Epoch[7] Avg accuracy: 0.79 Avg loss: 1.76\n",
      "Training Results - Epoch[8] Avg accuracy: 0.77 Avg loss: 1.77\n",
      "Training Results - Epoch[9] Avg accuracy: 0.82 Avg loss: 1.74\n",
      "Training Results - Epoch[10] Avg accuracy: 0.83 Avg loss: 1.72\n",
      "Training Results - Epoch[11] Avg accuracy: 0.83 Avg loss: 1.72\n",
      "Training Results - Epoch[12] Avg accuracy: 0.82 Avg loss: 1.73\n",
      "Training Results - Epoch[13] Avg accuracy: 0.83 Avg loss: 1.72\n",
      "Training Results - Epoch[14] Avg accuracy: 0.85 Avg loss: 1.70\n",
      "Training Results - Epoch[15] Avg accuracy: 0.84 Avg loss: 1.71\n",
      "Training Results - Epoch[16] Avg accuracy: 0.85 Avg loss: 1.70\n",
      "Training Results - Epoch[17] Avg accuracy: 0.85 Avg loss: 1.70\n",
      "Training Results - Epoch[18] Avg accuracy: 0.86 Avg loss: 1.69\n",
      "Training Results - Epoch[19] Avg accuracy: 0.85 Avg loss: 1.70\n",
      "Training Results - Epoch[20] Avg accuracy: 0.87 Avg loss: 1.68\n",
      "Training Results - Epoch[21] Avg accuracy: 0.87 Avg loss: 1.68\n",
      "Training Results - Epoch[22] Avg accuracy: 0.88 Avg loss: 1.68\n",
      "Training Results - Epoch[23] Avg accuracy: 0.88 Avg loss: 1.68\n",
      "Training Results - Epoch[24] Avg accuracy: 0.88 Avg loss: 1.67\n",
      "Training Results - Epoch[25] Avg accuracy: 0.87 Avg loss: 1.68\n",
      "Training Results - Epoch[26] Avg accuracy: 0.88 Avg loss: 1.67\n",
      "Training Results - Epoch[27] Avg accuracy: 0.90 Avg loss: 1.66\n",
      "Training Results - Epoch[28] Avg accuracy: 0.88 Avg loss: 1.67\n",
      "Training Results - Epoch[29] Avg accuracy: 0.88 Avg loss: 1.67\n",
      "Training Results - Epoch[30] Avg accuracy: 0.90 Avg loss: 1.66\n",
      "Training Results - Epoch[31] Avg accuracy: 0.90 Avg loss: 1.65\n",
      "Training Results - Epoch[32] Avg accuracy: 0.90 Avg loss: 1.65\n",
      "Training Results - Epoch[33] Avg accuracy: 0.90 Avg loss: 1.65\n",
      "Training Results - Epoch[34] Avg accuracy: 0.90 Avg loss: 1.65\n",
      "Training Results - Epoch[35] Avg accuracy: 0.91 Avg loss: 1.64\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = setup_dataflow(train_dataset, train_idx, val_idx)\n",
    "final_model, train_results, val_results = train_model(train_loader, val_loader, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8c976-0b8a-4467-9a34-e19bfb15924d",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13d3c7-8909-4a07-a187-bb7f9a2a723a",
   "metadata": {},
   "source": [
    "### Guardamos los pesos de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc384b-9522-4cd4-8305-24c65cccabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model.state_dict(), \"./weights/classifier.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5454d4-b398-4cf2-8a12-cb57de6aab9b",
   "metadata": {},
   "source": [
    "### Cargamos los pesos existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2586f09-367a-4d79-bc64-69a9484d77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = _C(input_h_w=64).to(device)\n",
    "final_model.load_state_dict(torch.load(\"./weights/classifier.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744429c-7453-4fa0-aa5f-ae6d3cd210ae",
   "metadata": {},
   "source": [
    "## Ploteamos las graficas de la función de perdida y del Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa2077-f0a2-4c10-88e1-bdb5217337fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-light.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f7eb6-437f-4429-a420-605102ee9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossses_train = [t['Loss'] for t in train_results]\n",
    "lossses_val = np.round([t['Loss'] for t in val_results], 3)\n",
    "\n",
    "plt.plot(range(np.shape(lossses_train)[0]),lossses_train, '-', label='Train')\n",
    "plt.plot(range(np.shape(lossses_val)[0]),lossses_val, '-', label='Val')\n",
    "plt.title('Funcion de Perdida')\n",
    "plt.xlabel('$Epoch$', fontsize=15)\n",
    "plt.ylabel('$BCE$ Loss', fontsize=15)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6175ca-c0c7-4413-9e30-2b0f9ce3a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = [t['Accuracy'] for t in train_results]\n",
    "acc_val = np.round([t['Accuracy'] for t in val_results], 3)\n",
    "\n",
    "plt.plot(range(np.shape(acc_train)[0]),acc_train, '-', label='Train')\n",
    "plt.plot(range(np.shape(acc_val)[0]),acc_val, '-', label='Val')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('$Epoch$', fontsize=15)\n",
    "plt.ylabel('$BCE$ Loss', fontsize=15)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a123963-e0e0-46fa-ac1b-bf7511a436b2",
   "metadata": {},
   "source": [
    "## Evaluamos nuestro modelo con la porción de datos de 'Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410f53a-f729-4d75-ba24-0fa5d2b572b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b87a2-2ca8-42b6-b8c7-c4591695bc3d",
   "metadata": {},
   "source": [
    "### Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668dc284-66d7-4731-a0c2-33e0c6745397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchnet\n",
    "\n",
    "confusion_matrix = torchnet.meter.ConfusionMeter(11, normalized=True)\n",
    "\n",
    "for ii, data_ in enumerate(test_loader):\n",
    "    input_, label = data_\n",
    "    val_input = input_.to(device)  # .cuda()\n",
    "    val_label = label.type(torch.LongTensor).to(device)  # .cuda()\n",
    "    score = final_model(val_input)\n",
    "    confusion_matrix.add(score.data.squeeze(), label.type(torch.LongTensor))\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print(confusion_matrix.conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d86cda4-3deb-45ed-8ef8-4e86f30ca224",
   "metadata": {},
   "source": [
    "### Recall, F1-Score, Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc67b1e-7c10-49d7-911d-c99a7857f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux import iterations_test\n",
    "from sklearn import metrics\n",
    "\n",
    "y_real, y_pred = iterations_test(final_model, test_loader, device)\n",
    "print(metrics.classification_report(np.array(y_pred), np.array(y_real)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fe8dc-a365-458c-b95d-4e096dc7f915",
   "metadata": {},
   "source": [
    "### Validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309c344-50ac-4b36-9612-f57d26604c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ConcatDataset([train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9474a23-f7fd-4f3a-93b7-ed9d09f4a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "splits = KFold(n_splits=num_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d567d2a-8dcc-4a0b-81fa-05ef17a522db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_per_fold = []\n",
    "\n",
    "for fold_idx, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "    nro_fold = fold_idx + 1\n",
    "    print('Fold {}'.format(nro_fold))\n",
    "    \n",
    "\n",
    "    train_loader, val_loader = setup_dataflow(dataset, train_idx, val_idx)\n",
    "    model, train_results, val_results = train_model(train_loader, val_loader, mode='Fold')\n",
    "    results_per_fold.append([train_results, val_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c435a8c-431c-4272-9212-2d99374a537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sum = 0\n",
    "for n_fold in range(len(results_per_fold)):\n",
    "  current_fold = results_per_fold[n_fold]\n",
    "  print(f\"Validation Results - Fold[{n_fold + 1}] Avg accuracy: {current_fold[1][2]['Accuracy']:.2f} Avg loss: {current_fold[1][2]['Loss']:.2f}\")\n",
    "  acc_sum += current_fold[1][2]['Accuracy']\n",
    "\n",
    "folds_mean = acc_sum/num_folds\n",
    "print(f\"Model validation average for {num_folds}-folds: {folds_mean :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4562da-bd3d-4407-a6d6-c1cfcb8224b1",
   "metadata": {},
   "source": [
    "##  Ejercicios\n",
    "\n",
    "- Visualizar las funciones de perdida para distintos learning rates. Qué observan en los gráficos?\n",
    "- Visualizar las funciones de perdida para distintos tamanos de lotes. \n",
    "- Visualizar un grupo de elementos del grupo de evaluacion con la clasificación y su puntaje respectivo.\n",
    "- Comparar metricas de red entrenada en 10, 50 y 100 epochs. Qué diferencias observan?\n",
    "- Re-entrenar un modelo preentrando `from torchvision import  models` para clasificacion, comparar sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba83e23-c9ac-4dc5-906b-b96cc576a97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c9099-7e95-42b1-b29c-ceab19a9de9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a90de-6e67-446f-92e5-92be6ec3cf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf45f3-98dd-47df-9dfd-10f778603a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
